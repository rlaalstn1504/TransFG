{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81014fcf-876a-48c8-b185-c04d6a625101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 텐서보드 테스트\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', scalar_value =np.random.random(), global_step=n_iter)\n",
    "    writer.add_scalar('Loss/test',  scalar_value =np.random.random(), global_step=n_iter)\n",
    "    writer.add_scalar('Accuracy/train',  scalar_value =np.random.random(), global_step=n_iter)\n",
    "    writer.add_scalar('Accuracy/test',  scalar_value =np.random.random(), global_step=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6334a082-c53d-4dcc-a6f7-510eb1c1418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, precision_score, recall_score\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image\n",
    "from datetime import timedelta\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from apex import amp\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
    "\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
    "from utils.data_utils import get_loader\n",
    "from utils.dist_util import get_world_size\n",
    "\n",
    "from utils.dataset import custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db86f763-b81c-4c4a-8d84-b97038f7af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([transforms.Resize((600, 600), Image.BILINEAR),\n",
    "                            transforms.CenterCrop((448, 448)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "testset = custom(root='/data/TransFG_experiment/datasets/custom', dtype=2, transform = test_transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7ed23f-cc07-4137-a8bc-89de57f7fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = SequentialSampler(testset)#if args.local_rank == -1 else DistributedSampler(testset) #SequentialSampler : 항상 같은 순서\n",
    "test_loader = DataLoader(testset,\n",
    "                             sampler=test_sampler,\n",
    "                             batch_size=40,\n",
    "                             num_workers=4,\n",
    "                             pin_memory=True) if testset is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de04c87f-5c85-4a1d-916d-9579f16ce396",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 448\n",
    "smoothing_value = 0.0\n",
    "pretrained_model = \"output/sample_run_checkpoint.bin\"\n",
    "config = CONFIGS[\"ViT-B_16\"] \n",
    "#config = CONFIGS[\"ViT-B_32\"]\n",
    "config.split = 'overlap'\n",
    "config.slide_step = 12\n",
    "num_classes = pd.read_csv('train_x.csv')['label'].nunique()\n",
    "model = VisionTransformer(config, img_size, num_classes, smoothing_value, zero_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d01b93-90bb-470f-9a04-32efedf3b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_model is not None:\n",
    "    pretrained_model = torch.load(pretrained_model, map_location=torch.device('cpu'))['model']\n",
    "    model.load_state_dict(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "734c0733-7cc4-4834-aea1-0ba16a420572",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (embeddings): Embeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(12, 12))\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (ffn): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn): Attention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (part_select): Part_Attention()\n",
       "      (part_layer): Block(\n",
       "        (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (part_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (part_head): Linear(in_features=768, out_features=430, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fcce57-1e7a-4bf7-aa49-d586808e4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2121 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch_iterator = tqdm(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d42c9b-eae5-4c07-a6b5-c5d347fa833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_label = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440cedc5-b308-4f86-9833-d97cb24db477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2121/2121 [55:10<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  0:55:09.464454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(epoch_iterator): \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y = batch\n",
    "        if len(y)==1:\n",
    "            print(y)\n",
    "            break\n",
    "        loss, logits = model(x, y)\n",
    "        #loss = loss.mean()\n",
    "        preds = torch.argmax(logits, dim=-1) \n",
    "        all_label.append(list(y.cpu().numpy()))\n",
    "        all_preds.append(list(preds.cpu().numpy())) \n",
    "end = time.time() \n",
    "print(\"Time elapsed: \", timedelta(seconds=end-start))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98503ef-c616-41ba-a6e8-3bdee4deeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.array(sum(all_preds,[]))\n",
    "all_label = np.array(sum(all_label,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0964ff2f-55de-4273-8265-faaba6a00e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84808,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c07793da-a652-4eaf-8f55-13f6dd814b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([389,  35,  57, 379,  98, 386,   5, 418,  86, 228])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[:10]#.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "699680fe-4814-401a-95bf-c8d6872c8540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([389,  35,  57, 379,  98, 386,   7, 418,  86, 228])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f710ceb3-bdcb-4bb0-8354-73ac3d3516b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8821337609659466\n",
      "Precision : 0.8765267034400398\n",
      "Recall : 0.8821337609659466\n",
      "F1 score : 0.8744986230971883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :',accuracy_score(all_label, all_preds)) \n",
    "print('Precision :',precision_score(all_label, all_preds, average='weighted')) \n",
    "print('Recall :',recall_score(all_label, all_preds, average='weighted')) \n",
    "print('F1 score :',f1_score(all_label, all_preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd2d0f0b-976d-4b1c-986c-72a49a0ca0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       103\n",
      "           1       0.75      0.42      0.54        36\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.67      0.39      0.49        36\n",
      "           4       0.71      0.48      0.57        46\n",
      "           5       0.55      0.66      0.60       253\n",
      "           6       0.53      0.34      0.42       102\n",
      "           7       0.46      0.28      0.35       163\n",
      "           8       1.00      0.50      0.67        14\n",
      "           9       0.81      0.62      0.70        42\n",
      "          10       0.66      0.78      0.71       329\n",
      "          11       0.65      0.83      0.73       521\n",
      "          12       0.40      0.05      0.09        39\n",
      "          13       0.00      0.00      0.00        10\n",
      "          14       0.66      0.35      0.46        82\n",
      "          15       0.62      0.44      0.51       157\n",
      "          16       0.50      0.40      0.44        25\n",
      "          17       0.50      0.11      0.18        46\n",
      "          18       0.47      0.61      0.53        49\n",
      "          19       0.50      0.63      0.56        92\n",
      "          20       0.65      0.26      0.37        42\n",
      "          21       0.44      0.43      0.44        51\n",
      "          22       0.67      0.42      0.52        19\n",
      "          23       0.71      0.82      0.76       108\n",
      "          24       0.55      0.78      0.64       108\n",
      "          25       0.40      0.17      0.24        12\n",
      "          26       0.57      0.32      0.41        38\n",
      "          27       0.76      0.59      0.67        54\n",
      "          28       0.84      0.82      0.83        62\n",
      "          29       0.91      0.95      0.93       173\n",
      "          30       0.93      0.93      0.93       449\n",
      "          31       0.84      0.70      0.77       100\n",
      "          32       0.84      0.79      0.81       187\n",
      "          33       0.88      0.93      0.90       352\n",
      "          34       0.00      0.00      0.00         6\n",
      "          35       0.95      0.95      0.95      1080\n",
      "          36       0.97      0.97      0.97       739\n",
      "          37       0.91      0.90      0.90       712\n",
      "          38       0.74      0.71      0.72       268\n",
      "          39       0.91      0.92      0.91       225\n",
      "          40       0.84      0.85      0.84       287\n",
      "          41       0.92      0.96      0.94       594\n",
      "          42       0.95      0.94      0.95       370\n",
      "          43       0.96      0.97      0.97       321\n",
      "          44       0.50      0.42      0.46        43\n",
      "          45       0.90      0.82      0.86        44\n",
      "          46       0.64      0.44      0.52        36\n",
      "          47       0.94      0.95      0.94       150\n",
      "          48       0.85      0.83      0.84       145\n",
      "          49       0.87      0.93      0.90       397\n",
      "          50       0.83      0.76      0.79       269\n",
      "          51       0.95      0.75      0.84       118\n",
      "          52       0.00      0.00      0.00        26\n",
      "          53       0.91      0.96      0.93       620\n",
      "          54       0.94      0.92      0.93       687\n",
      "          55       0.71      0.72      0.72        65\n",
      "          56       0.83      0.80      0.81       113\n",
      "          57       0.92      0.98      0.95       413\n",
      "          58       0.81      0.45      0.58        38\n",
      "          59       0.95      0.93      0.94       879\n",
      "          60       0.94      0.96      0.95      1277\n",
      "          61       0.93      0.97      0.95       163\n",
      "          62       0.94      0.98      0.96       339\n",
      "          63       0.00      0.00      0.00        44\n",
      "          64       0.00      0.00      0.00         1\n",
      "          65       0.92      0.96      0.94      2547\n",
      "          66       0.00      0.00      0.00         9\n",
      "          67       0.96      0.96      0.96       490\n",
      "          68       0.92      0.92      0.92       103\n",
      "          69       0.71      0.79      0.75        70\n",
      "          70       0.96      0.96      0.96       778\n",
      "          71       0.93      0.96      0.95       280\n",
      "          72       0.97      0.98      0.97       724\n",
      "          73       0.89      0.94      0.91       198\n",
      "          74       0.97      0.96      0.97       899\n",
      "          75       0.92      0.85      0.88       226\n",
      "          76       0.89      0.94      0.92       392\n",
      "          77       0.97      0.98      0.97      1530\n",
      "          78       0.87      0.87      0.87       112\n",
      "          79       0.83      0.56      0.67        18\n",
      "          80       0.85      0.65      0.74        43\n",
      "          81       0.00      0.00      0.00         8\n",
      "          82       0.74      0.82      0.78        17\n",
      "          83       0.89      0.97      0.92       145\n",
      "          84       0.62      0.36      0.46        22\n",
      "          85       0.93      0.97      0.95       303\n",
      "          86       0.96      0.97      0.96       684\n",
      "          87       0.96      0.97      0.97       432\n",
      "          88       1.00      0.17      0.29        12\n",
      "          89       0.77      0.75      0.76       633\n",
      "          90       0.83      0.87      0.85      1012\n",
      "          91       0.90      0.93      0.92       193\n",
      "          92       0.83      0.29      0.43        35\n",
      "          93       0.77      0.65      0.70        88\n",
      "          94       0.80      0.57      0.67        28\n",
      "          95       0.84      0.95      0.89       204\n",
      "          96       1.00      0.15      0.27        13\n",
      "          97       0.77      0.55      0.64        42\n",
      "          98       0.88      0.92      0.90        87\n",
      "          99       0.69      0.61      0.65        57\n",
      "         100       0.69      0.65      0.67        17\n",
      "         101       0.78      0.79      0.78        62\n",
      "         102       1.00      0.25      0.40        16\n",
      "         103       0.79      0.87      0.83        61\n",
      "         104       0.84      0.87      0.85        83\n",
      "         105       1.00      0.81      0.89        26\n",
      "         106       0.84      0.81      0.83       172\n",
      "         107       0.76      0.84      0.80        96\n",
      "         108       0.91      0.77      0.83        39\n",
      "         109       0.83      0.88      0.85        72\n",
      "         110       0.73      0.47      0.57        17\n",
      "         111       0.80      0.84      0.82        58\n",
      "         112       0.79      0.61      0.69        49\n",
      "         113       0.45      0.78      0.57        32\n",
      "         114       0.85      0.90      0.88       135\n",
      "         115       0.73      0.86      0.79        59\n",
      "         116       0.77      0.63      0.69        70\n",
      "         117       0.71      0.19      0.30        26\n",
      "         118       0.76      0.75      0.75        59\n",
      "         119       0.86      0.25      0.39        24\n",
      "         120       0.82      0.93      0.87       290\n",
      "         121       0.74      0.78      0.76       147\n",
      "         122       0.00      0.00      0.00        19\n",
      "         123       0.75      0.52      0.61        29\n",
      "         124       0.00      0.00      0.00        16\n",
      "         125       0.75      0.86      0.80       124\n",
      "         126       0.55      0.36      0.43        45\n",
      "         127       0.93      0.95      0.94       330\n",
      "         128       0.69      0.69      0.69       104\n",
      "         129       0.64      0.61      0.62        98\n",
      "         130       0.96      0.96      0.96       866\n",
      "         131       0.68      0.75      0.71       100\n",
      "         132       0.90      0.72      0.80       127\n",
      "         133       0.87      0.89      0.88       343\n",
      "         134       0.00      0.00      0.00        12\n",
      "         135       0.79      0.85      0.82       408\n",
      "         136       0.81      0.86      0.84       127\n",
      "         137       0.86      0.73      0.79       177\n",
      "         138       0.79      0.84      0.81       420\n",
      "         139       0.87      0.80      0.83       220\n",
      "         140       0.93      0.96      0.95       535\n",
      "         141       0.85      0.93      0.89       200\n",
      "         142       0.64      0.84      0.73        86\n",
      "         143       0.91      0.75      0.83        57\n",
      "         144       0.78      0.84      0.81       188\n",
      "         145       0.89      0.53      0.67        45\n",
      "         146       0.77      0.79      0.78       128\n",
      "         147       0.69      0.55      0.61       146\n",
      "         148       0.78      0.89      0.83       157\n",
      "         149       0.90      0.70      0.79       188\n",
      "         150       0.83      0.94      0.88       413\n",
      "         151       0.90      0.89      0.90       759\n",
      "         152       0.48      0.32      0.38        41\n",
      "         153       0.59      0.53      0.56        90\n",
      "         154       1.00      0.36      0.53        11\n",
      "         155       0.79      0.50      0.61        30\n",
      "         156       0.79      0.81      0.80        74\n",
      "         157       0.66      0.64      0.65       235\n",
      "         158       0.52      0.46      0.48       103\n",
      "         159       0.51      0.52      0.51        64\n",
      "         160       0.46      0.46      0.46        85\n",
      "         161       0.00      0.00      0.00        14\n",
      "         162       1.00      0.19      0.32        26\n",
      "         163       0.00      0.00      0.00         8\n",
      "         164       0.71      0.65      0.68        31\n",
      "         165       0.81      0.88      0.84       329\n",
      "         166       0.81      0.88      0.84       840\n",
      "         167       0.95      0.46      0.62        41\n",
      "         168       0.79      0.81      0.80        77\n",
      "         169       0.58      0.50      0.54        58\n",
      "         170       0.71      0.81      0.76       160\n",
      "         171       0.72      0.74      0.73        69\n",
      "         172       0.69      0.85      0.76        95\n",
      "         173       0.00      0.00      0.00        13\n",
      "         174       0.00      0.00      0.00        16\n",
      "         175       0.00      0.00      0.00         6\n",
      "         176       0.60      0.60      0.60        35\n",
      "         177       0.79      0.75      0.77       237\n",
      "         178       0.80      0.77      0.78       166\n",
      "         179       0.00      0.00      0.00        11\n",
      "         180       0.50      0.50      0.50        12\n",
      "         181       0.00      0.00      0.00        12\n",
      "         182       0.69      0.82      0.75        50\n",
      "         183       0.84      0.87      0.85        76\n",
      "         184       0.17      0.06      0.09        17\n",
      "         185       0.72      0.40      0.51        98\n",
      "         186       0.81      0.86      0.84        71\n",
      "         187       0.61      0.78      0.69       127\n",
      "         188       0.61      0.66      0.64        41\n",
      "         189       0.00      0.00      0.00        10\n",
      "         190       0.70      0.82      0.76        76\n",
      "         191       0.00      0.00      0.00         1\n",
      "         192       0.95      0.96      0.96       226\n",
      "         193       0.95      0.89      0.92       157\n",
      "         194       0.00      0.00      0.00         5\n",
      "         195       0.00      0.00      0.00         5\n",
      "         196       0.93      0.93      0.93       122\n",
      "         197       0.92      0.71      0.80        79\n",
      "         198       0.95      0.90      0.92       296\n",
      "         199       0.85      0.94      0.89       333\n",
      "         200       0.00      0.00      0.00         4\n",
      "         201       0.00      0.00      0.00         3\n",
      "         202       0.91      0.91      0.91        43\n",
      "         203       0.89      0.89      0.89       501\n",
      "         204       0.83      0.79      0.81       239\n",
      "         205       0.97      0.99      0.98       844\n",
      "         206       0.00      0.00      0.00         6\n",
      "         207       0.00      0.00      0.00         3\n",
      "         208       0.00      0.00      0.00        12\n",
      "         209       0.67      0.75      0.71        24\n",
      "         210       0.92      0.95      0.94       123\n",
      "         211       0.97      0.96      0.96       376\n",
      "         212       0.00      0.00      0.00        10\n",
      "         213       0.69      0.92      0.79       102\n",
      "         214       0.67      0.56      0.61        25\n",
      "         215       0.84      0.81      0.82        67\n",
      "         216       0.00      0.00      0.00         3\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         5\n",
      "         219       0.84      0.69      0.76       104\n",
      "         220       1.00      0.25      0.40         8\n",
      "         221       0.00      0.00      0.00         8\n",
      "         222       0.81      0.68      0.74        73\n",
      "         223       0.71      0.60      0.65        40\n",
      "         224       0.83      0.95      0.89       270\n",
      "         225       0.83      0.80      0.82        56\n",
      "         226       0.79      0.70      0.74        37\n",
      "         227       0.86      0.82      0.84       182\n",
      "         228       0.76      0.84      0.80       139\n",
      "         229       0.93      0.96      0.94       187\n",
      "         230       0.87      0.92      0.89       165\n",
      "         231       0.77      0.91      0.84       125\n",
      "         232       0.70      0.65      0.68        66\n",
      "         233       0.82      0.48      0.61        29\n",
      "         234       0.83      0.79      0.81        62\n",
      "         235       0.73      0.44      0.55        73\n",
      "         236       0.79      0.92      0.85       217\n",
      "         237       0.95      0.76      0.84        25\n",
      "         238       1.00      0.62      0.77        16\n",
      "         239       0.00      0.00      0.00         3\n",
      "         240       0.00      0.00      0.00         4\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.62      0.47      0.54        32\n",
      "         243       0.81      0.77      0.79        44\n",
      "         244       1.00      0.09      0.17        11\n",
      "         245       0.68      0.75      0.71        40\n",
      "         246       0.80      0.83      0.81        42\n",
      "         247       0.00      0.00      0.00        10\n",
      "         248       0.76      0.67      0.71        24\n",
      "         249       0.79      0.87      0.83       149\n",
      "         250       0.83      0.77      0.80       102\n",
      "         251       0.94      0.52      0.67        29\n",
      "         252       1.00      0.67      0.80         9\n",
      "         253       0.89      0.92      0.90       160\n",
      "         254       0.89      0.95      0.92       231\n",
      "         255       0.95      0.93      0.94        87\n",
      "         256       0.66      0.74      0.70       144\n",
      "         257       0.78      0.65      0.71       206\n",
      "         258       0.87      0.88      0.87       517\n",
      "         259       0.71      0.27      0.39        56\n",
      "         260       0.55      0.19      0.28        94\n",
      "         261       0.57      0.48      0.53        64\n",
      "         262       0.41      0.40      0.40        73\n",
      "         263       0.63      0.72      0.67       174\n",
      "         264       0.57      0.84      0.68       258\n",
      "         265       0.75      0.55      0.63        33\n",
      "         266       0.42      0.35      0.38        68\n",
      "         267       1.00      0.05      0.09        21\n",
      "         268       0.79      0.42      0.55        26\n",
      "         269       0.57      0.64      0.60        25\n",
      "         270       0.75      0.12      0.21        25\n",
      "         271       0.66      0.80      0.73        82\n",
      "         272       0.65      0.74      0.69        38\n",
      "         273       0.00      0.00      0.00        12\n",
      "         274       0.52      0.87      0.65        15\n",
      "         275       0.00      0.00      0.00         9\n",
      "         276       0.00      0.00      0.00         5\n",
      "         277       0.00      0.00      0.00         4\n",
      "         278       0.00      0.00      0.00         7\n",
      "         279       1.00      0.09      0.17        11\n",
      "         280       0.67      0.18      0.29        11\n",
      "         281       0.73      0.70      0.72        27\n",
      "         282       0.00      0.00      0.00         1\n",
      "         283       0.00      0.00      0.00         8\n",
      "         284       0.80      0.98      0.88        52\n",
      "         285       0.00      0.00      0.00         9\n",
      "         286       0.97      0.95      0.96        62\n",
      "         287       0.71      0.71      0.71        35\n",
      "         288       0.56      0.64      0.60        28\n",
      "         289       1.00      0.12      0.22        24\n",
      "         290       0.00      0.00      0.00         1\n",
      "         291       0.00      0.00      0.00         6\n",
      "         292       0.00      0.00      0.00         4\n",
      "         293       0.79      0.79      0.79        14\n",
      "         294       0.00      0.00      0.00        10\n",
      "         295       1.00      0.71      0.83         7\n",
      "         296       0.74      0.81      0.77        52\n",
      "         297       0.92      0.67      0.77        54\n",
      "         298       0.82      0.98      0.89       253\n",
      "         299       0.62      0.10      0.18        49\n",
      "         300       0.00      0.00      0.00         5\n",
      "         301       0.00      0.00      0.00        10\n",
      "         302       0.85      0.89      0.87       105\n",
      "         303       0.73      0.74      0.74        74\n",
      "         304       1.00      0.13      0.24        15\n",
      "         305       1.00      0.29      0.44        14\n",
      "         306       0.67      0.44      0.53        18\n",
      "         307       0.70      0.89      0.78       117\n",
      "         308       0.80      0.11      0.20        35\n",
      "         309       0.62      0.67      0.64        42\n",
      "         310       0.59      0.64      0.62        64\n",
      "         311       0.52      0.63      0.57        19\n",
      "         312       0.00      0.00      0.00         7\n",
      "         313       0.80      0.76      0.78        86\n",
      "         314       0.78      0.89      0.83       112\n",
      "         315       0.00      0.00      0.00         8\n",
      "         316       0.67      0.25      0.36         8\n",
      "         317       0.65      0.85      0.73        13\n",
      "         318       1.00      0.50      0.67        12\n",
      "         319       0.88      0.87      0.87        60\n",
      "         320       0.64      0.75      0.69        36\n",
      "         321       0.64      0.53      0.58        60\n",
      "         322       0.89      0.95      0.92       181\n",
      "         323       0.90      0.93      0.91       207\n",
      "         324       0.77      0.42      0.55        40\n",
      "         325       0.85      0.61      0.71        18\n",
      "         326       0.64      0.71      0.67        86\n",
      "         327       0.82      0.84      0.83        37\n",
      "         328       0.75      0.19      0.30        16\n",
      "         329       0.40      0.22      0.29        18\n",
      "         330       0.75      0.23      0.35        13\n",
      "         331       0.84      0.66      0.74        47\n",
      "         332       0.91      0.92      0.92       190\n",
      "         333       0.57      0.51      0.54        41\n",
      "         334       0.71      0.77      0.74       104\n",
      "         335       0.70      0.80      0.75        50\n",
      "         336       0.84      0.78      0.81        65\n",
      "         337       0.77      0.21      0.33        47\n",
      "         338       0.97      0.98      0.97      1696\n",
      "         339       0.95      0.95      0.95      1849\n",
      "         340       0.94      0.97      0.96       739\n",
      "         341       0.90      0.76      0.82        62\n",
      "         342       0.00      0.00      0.00         2\n",
      "         343       0.94      0.99      0.96      1470\n",
      "         344       0.95      0.90      0.93        42\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       0.00      0.00      0.00        10\n",
      "         347       0.81      0.86      0.83       176\n",
      "         348       0.88      0.85      0.87       136\n",
      "         349       0.78      0.33      0.47        42\n",
      "         350       0.82      0.82      0.82        84\n",
      "         351       0.57      0.48      0.52        58\n",
      "         352       0.84      0.92      0.88       253\n",
      "         353       0.94      0.96      0.95       194\n",
      "         354       0.97      0.99      0.98       283\n",
      "         355       0.00      0.00      0.00        13\n",
      "         356       0.00      0.00      0.00         7\n",
      "         357       0.83      0.90      0.86        69\n",
      "         358       0.83      0.81      0.82        42\n",
      "         359       0.96      0.88      0.92       426\n",
      "         360       0.96      0.98      0.97      1540\n",
      "         361       0.78      0.84      0.81        45\n",
      "         362       0.94      0.98      0.96       223\n",
      "         363       0.97      0.97      0.97       658\n",
      "         364       0.93      0.93      0.93      1602\n",
      "         365       0.93      0.92      0.92       872\n",
      "         366       0.89      0.91      0.90       410\n",
      "         367       0.87      0.84      0.86        70\n",
      "         368       0.96      0.98      0.97       756\n",
      "         369       0.84      0.88      0.86        59\n",
      "         370       0.94      0.93      0.94      1350\n",
      "         371       0.95      0.95      0.95       794\n",
      "         372       0.90      0.77      0.83       112\n",
      "         373       0.94      0.95      0.94      1029\n",
      "         374       0.97      0.97      0.97      2189\n",
      "         375       0.00      0.00      0.00        10\n",
      "         376       0.92      0.93      0.92       806\n",
      "         377       0.96      0.98      0.97       651\n",
      "         378       0.95      0.98      0.97       599\n",
      "         379       0.95      0.96      0.95      1382\n",
      "         380       0.80      0.93      0.86       105\n",
      "         381       0.95      0.91      0.93       299\n",
      "         382       0.88      0.62      0.73        45\n",
      "         383       0.97      0.97      0.97       360\n",
      "         384       0.57      0.51      0.54        72\n",
      "         385       0.65      0.67      0.66        88\n",
      "         386       0.87      0.93      0.90       256\n",
      "         387       0.86      0.97      0.91       116\n",
      "         388       0.75      0.17      0.27        54\n",
      "         389       0.81      0.90      0.85       299\n",
      "         390       0.95      0.87      0.91        46\n",
      "         391       0.74      0.25      0.38       231\n",
      "         392       0.78      0.89      0.83       168\n",
      "         393       0.91      0.87      0.89       215\n",
      "         394       0.91      0.95      0.93       764\n",
      "         395       0.79      0.90      0.84       819\n",
      "         396       0.86      0.70      0.78        88\n",
      "         397       0.87      0.92      0.89       232\n",
      "         398       0.85      0.82      0.84        34\n",
      "         399       0.85      0.96      0.90       284\n",
      "         400       0.96      0.93      0.94       445\n",
      "         401       0.76      0.80      0.78        46\n",
      "         402       0.91      0.94      0.93       449\n",
      "         403       0.00      0.00      0.00         6\n",
      "         404       0.75      0.69      0.72        13\n",
      "         405       0.97      0.98      0.97       127\n",
      "         406       0.82      0.66      0.73        47\n",
      "         407       0.87      0.90      0.88        88\n",
      "         408       0.93      0.97      0.95       505\n",
      "         409       1.00      0.57      0.73        14\n",
      "         410       0.00      0.00      0.00         1\n",
      "         411       0.00      0.00      0.00         3\n",
      "         412       0.96      0.98      0.97       318\n",
      "         413       0.86      0.95      0.90       433\n",
      "         414       0.89      0.72      0.80       293\n",
      "         415       0.89      0.96      0.92       803\n",
      "         416       0.90      0.90      0.90       125\n",
      "         417       0.90      0.90      0.90        62\n",
      "         418       0.82      0.99      0.90       748\n",
      "         419       0.67      0.01      0.03       146\n",
      "         420       1.00      0.06      0.12        79\n",
      "         421       0.92      0.95      0.94      2680\n",
      "         422       0.70      0.39      0.50        18\n",
      "         423       0.76      0.69      0.72        49\n",
      "         424       0.59      0.73      0.65        45\n",
      "         425       0.00      0.00      0.00         5\n",
      "         426       0.00      0.00      0.00         3\n",
      "         427       1.00      0.77      0.87        22\n",
      "         428       0.60      0.32      0.41        19\n",
      "         429       0.82      0.81      0.81       154\n",
      "\n",
      "    accuracy                           0.88     84808\n",
      "   macro avg       0.68      0.61      0.63     84808\n",
      "weighted avg       0.88      0.88      0.87     84808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_label, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38f913-4266-4ebf-9ff6-ddf5b0098931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e280b33-20d9-4306-af9f-7807e0bcdc48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
